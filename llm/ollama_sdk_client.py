"""
ComfyUI-SiberiaNodes - Complete Ollama SDK-based client implementation

Author: siberiah0h
Email: siberiah0h@gmail.com
Technical Blog: www.dataeast.cn
Last Updated: 2025-11-15
"""

import re
import tempfile
import os
import base64
import io
from typing import Dict, List, Tuple, Optional, Union
import torch
import numpy as np
from ollama import Client, ResponseError, RequestError

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("Warning: PIL not available, image processing will be limited")


class SiberiaOllamaSDKClient:
    """
    Siberia Ollama SDK Client - å®Œå…¨åŸºäºOllamaå®˜æ–¹SDKçš„å®¢æˆ·ç«¯
    Siberia Ollama SDK Client - Client based entirely on official Ollama SDK
    """

    # æ”¯æŒè§†è§‰çš„å·²çŸ¥æ¨¡å‹å…³é”®è¯å’Œå®Œæ•´æ¨¡å‹å
    VISION_MODEL_KEYWORDS = [
        'vision', 'vl', 'multimodal', 'llava', 'bakllava', 'moondream',
        'qwen2-vl', 'qwen-vl', 'llama3.2-vision', 'minicpm-v',
        'cogvlm', 'internvl', 'xverse-v'
    ]

    VISION_MODELS_EXACT = [
        'llava:latest', 'llava:13b', 'llava:34b', 'llava:7b',
        'bakllava:latest', 'moondream:latest', 'qwen2-vl:latest',
        'qwen2-vl:7b', 'qwen2-vl:2b', 'llama3.2-vision:latest',
        'llama3.2-vision:11b', 'llama3.2-vision:90b'
    ]

    def __init__(self, server_url: str = "http://127.0.0.1:11434", model: str = "llama2", timeout: int = 30,
                 use_base64: bool = True):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ / Initialize client

        Args:
            server_url: OllamaæœåŠ¡å™¨URL
            model: é»˜è®¤æ¨¡å‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
            use_base64: æ˜¯å¦ä½¿ç”¨base64æ ¼å¼ä¼ è¾“å›¾ç‰‡
        """
        self.server_url = self._normalize_server_url(server_url)
        self.model = model
        self.timeout = max(5, min(300, int(timeout)))  # é™åˆ¶åœ¨5-300ç§’ä¹‹é—´
        self.use_base64 = use_base64

        # è¿æ¥çŠ¶æ€
        self._connected = False
        self._available_models = []

        # åˆ›å»ºOllama SDKå®¢æˆ·ç«¯å®ä¾‹ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
        self._client = None

    def _normalize_server_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–æœåŠ¡å™¨URL / Normalize server URL"""
        if not url or not isinstance(url, str):
            return "http://127.0.0.1:11434"

        # ç§»é™¤å°¾éƒ¨æ–œæ 
        url = url.rstrip('/')

        # å¦‚æœæ²¡æœ‰åè®®å‰ç¼€ï¼Œæ·»åŠ http://
        if not url.startswith(('http://', 'https://')):
            url = f'http://{url}'

        # åŸºæœ¬URLæ ¼å¼éªŒè¯
        pattern = r'^https?://[a-zA-Z0-9.-]+(?::\d{1,5})?$'
        if not re.match(pattern, url):
            print(f"Warning: Invalid URL format '{url}', using default")
            return "http://127.0.0.1:11434"

        return url

    def _get_client(self) -> Client:
        """è·å–Ollama SDKå®¢æˆ·ç«¯å®ä¾‹ / Get Ollama SDK client instance"""
        if self._client is None:
            # Ollama SDK éœ€è¦ä¸»æœºéƒ¨åˆ†ï¼Œä¸åŒ…å«åè®®
            host = self._extract_host_from_url(self.server_url)
            self._client = Client(host=host, timeout=self.timeout)
        return self._client

    def _extract_host_from_url(self, url: str) -> str:
        """ä»URLæå–ä¸»æœºéƒ¨åˆ† / Extract host part from URL"""
        if url.startswith('http://'):
            return url[7:]
        elif url.startswith('https://'):
            return url[8:]
        else:
            return url

    def is_vision_model(self, model_name: str = None) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½ / Check if model supports vision capabilities

        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ¨¡å‹

        Returns:
            bool: æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½
        """
        if model_name is None:
            model_name = self.model

        if not model_name:
            return False

        model_name_lower = model_name.lower()

        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if model_name_lower in [m.lower() for m in self.VISION_MODELS_EXACT]:
            return True

        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        return any(keyword in model_name_lower for keyword in self.VISION_MODEL_KEYWORDS)

    @classmethod
    def is_vision_model_static(cls, model_name: str) -> bool:
        """
        é™æ€æ–¹æ³•æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½ / Static method to check if model supports vision

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            bool: æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½
        """
        if not model_name:
            return False

        model_name_lower = model_name.lower()

        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if model_name_lower in [m.lower() for m in cls.VISION_MODELS_EXACT]:
            return True

        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        return any(keyword in model_name_lower for keyword in cls.VISION_MODEL_KEYWORDS)

    def test_connection(self) -> bool:
        """
        æµ‹è¯•è¿æ¥å¹¶è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ / Test connection and get available models

        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"Testing connection to: {self.server_url}")

            client = self._get_client()

            # ä½¿ç”¨Ollama SDKçš„listæ–¹æ³•è·å–æ¨¡å‹
            models_response = client.list()

            # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
            models = []
            if isinstance(models_response, dict):
                models = models_response.get('models', [])
            elif hasattr(models_response, 'models'):
                models = models_response.models

            # æå–æ¨¡å‹åç§°
            self._available_models = []
            for model in models:
                if isinstance(model, dict):
                    name = model.get('name', '') or model.get('model', '')
                elif hasattr(model, 'name'):
                    name = model.name
                elif hasattr(model, 'model'):
                    name = model.model
                else:
                    continue

                if name:  # ç¡®ä¿åç§°ä¸ä¸ºç©º
                    self._available_models.append(name)

            self._connected = True
            print(f"Connection successful. Found {len(self._available_models)} models")

            if self._available_models:
                print(f"Available models: {', '.join(self._available_models[:5])}" +
                      (f" and {len(self._available_models) - 5} more..." if len(self._available_models) > 5 else ""))

            return True

        except (ResponseError, RequestError) as e:
            error_msg = f"Ollama API error: {e}"
            print(error_msg)
        except Exception as e:
            error_msg = f"Connection error: {type(e).__name__}: {e}"
            print(error_msg)

        self._connected = False
        self._available_models = []
        return False

    def generate_text(self, prompt: str, system_prompt: str = "You are a helpful assistant.",
                     temperature: float = 0.7, max_tokens: int = 500) -> Tuple[str, str]:
        """
        ç”Ÿæˆæ–‡æœ¬ / Generate text

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: ç”Ÿæˆæ¸©åº¦ (0.0-2.0)
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            Tuple[str, str]: (ç”Ÿæˆçš„æ–‡æœ¬, çŠ¶æ€ä¿¡æ¯)
        """
        try:
            # éªŒè¯è¾“å…¥
            if not prompt or not prompt.strip():
                return "", "Error: Empty prompt"

            # é™åˆ¶å‚æ•°èŒƒå›´
            temperature = max(0.0, min(2.0, float(temperature)))
            max_tokens = max(1, min(8192, int(max_tokens)))

            # æ£€æŸ¥è¿æ¥
            if not self._connected:
                if not self.test_connection():
                    return "", "Error: Failed to connect to Ollama server"

            if not self._available_models:
                return "", "Error: No models available on server"

            client = self._get_client()
            print(f"Generating text with model: {self.model}")

            # ä½¿ç”¨Ollama SDKç”Ÿæˆæ–‡æœ¬
            response = client.generate(
                model=self.model,
                prompt=prompt.strip(),
                system=system_prompt.strip(),
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )

            # å¤„ç†å“åº”
            generated_text = ""
            if isinstance(response, dict):
                generated_text = response.get('response', '')
            elif hasattr(response, 'response'):
                generated_text = response.response

            if generated_text:
                status_msg = f"Successfully generated {len(generated_text)} characters"
                return generated_text, status_msg
            else:
                return "", "Error: Empty response from model"

        except (ResponseError, RequestError) as e:
            error_msg = f"Ollama API error: {e}"
            return "", error_msg
        except Exception as e:
            error_msg = f"Generation error: {type(e).__name__}: {e}"
            return "", error_msg

    def chat(self, messages: List[Dict], temperature: float = 0.7, max_tokens: int = 4096) -> Tuple[str, str, List[Dict]]:
        """
        èŠå¤©å¯¹è¯ / Chat conversation

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨
            temperature: ç”Ÿæˆæ¸©åº¦ (0.0-2.0)
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            Tuple[str, str, List[Dict]]: (å›å¤æ–‡æœ¬, çŠ¶æ€ä¿¡æ¯, æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨)
        """
        try:
            # éªŒè¯è¾“å…¥
            if not messages or not isinstance(messages, list):
                return "", "Error: Invalid or empty messages", []

            # éªŒè¯æ¶ˆæ¯æ ¼å¼
            valid_roles = {'system', 'user', 'assistant'}
            for msg in messages:
                if not isinstance(msg, dict) or 'role' not in msg or 'content' not in msg:
                    return "", "Error: Invalid message format", messages
                if msg['role'] not in valid_roles:
                    return "", f"Error: Invalid role '{msg['role']}'", messages

            # é™åˆ¶å‚æ•°èŒƒå›´
            temperature = max(0.0, min(2.0, float(temperature)))
            max_tokens = max(1, min(8192, int(max_tokens)))

            # æ£€æŸ¥è¿æ¥
            if not self._connected:
                if not self.test_connection():
                    return "", "Error: Failed to connect to Ollama server", messages

            if not self._available_models:
                return "", "Error: No models available on server", messages

            client = self._get_client()
            print(f"Chat request with {len(messages)} messages using model: {self.model}")

            # ä½¿ç”¨Ollama SDKè¿›è¡ŒèŠå¤©
            response = client.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )

            # å¤„ç†å“åº”
            response_text = ""
            if isinstance(response, dict):
                message = response.get('message', {})
                if isinstance(message, dict):
                    response_text = message.get('content', '')
            elif hasattr(response, 'message') and hasattr(response.message, 'content'):
                response_text = response.message.content

            if response_text:
                # æ›´æ–°æ¶ˆæ¯å†å²
                updated_messages = messages + [{"role": "assistant", "content": response_text}]
                status_msg = f"Chat successful: {len(response_text)} characters generated"
                return response_text, status_msg, updated_messages
            else:
                return "", "Error: Empty response from model", messages

        except (ResponseError, RequestError) as e:
            error_msg = f"Ollama API error: {e}"
            return "", error_msg, messages
        except Exception as e:
            error_msg = f"Chat error: {type(e).__name__}: {e}"
            return "", error_msg, messages

    def analyze_image(self, prompt: str, image_data, system_prompt: str = "You are a helpful assistant.",
                     temperature: float = 0.7, max_tokens: int = 500) -> Tuple[str, str]:
        """
        åˆ†æå›¾ç‰‡ / Analyze image

        Args:
            prompt: å›¾ç‰‡åˆ†ææç¤ºè¯
            image_data: å›¾ç‰‡æ•°æ® (torch.Tensoræˆ–æ–‡ä»¶è·¯å¾„)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: ç”Ÿæˆæ¸©åº¦ (0.0-2.0)
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            Tuple[str, str]: (åˆ†æç»“æœ, çŠ¶æ€ä¿¡æ¯)
        """
        try:
            # éªŒè¯è¾“å…¥
            if not prompt or not prompt.strip():
                return "", "Error: Empty prompt"

            if image_data is None:
                return "", "Error: No image data provided"

            # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½
            if not self.is_vision_model():
                return "", f"Error: Model '{self.model}' does not support vision. Please use a vision model."

            # é™åˆ¶å‚æ•°èŒƒå›´
            temperature = max(0.0, min(2.0, float(temperature)))
            max_tokens = max(1, min(8192, int(max_tokens)))

            # æ£€æŸ¥è¿æ¥
            if not self._connected:
                if not self.test_connection():
                    return "", "Error: Failed to connect to Ollama server"

            if not self._available_models:
                return "", "Error: No models available on server"

            # å‡†å¤‡å›¾ç‰‡æ•°æ®
            image_data_processed = self._prepare_image_for_sdk(image_data)
            if not image_data_processed:
                return "", "Error: Failed to prepare image for analysis"

            try:
                client = self._get_client()
                print(f"Analyzing image with model: {self.model} (format: {'base64' if self.use_base64 else 'file path'})")

                # å‡†å¤‡æ¶ˆæ¯
                if self.use_base64:
                    # ä½¿ç”¨base64æ•°æ®
                    messages = [
                        {
                            'role': 'system',
                            'content': system_prompt.strip()
                        },
                        {
                            'role': 'user',
                            'content': prompt.strip(),
                            'images': [image_data_processed]
                        }
                    ]
                else:
                    # ä½¿ç”¨æ–‡ä»¶è·¯å¾„
                    messages = [
                        {
                            'role': 'system',
                            'content': system_prompt.strip()
                        },
                        {
                            'role': 'user',
                            'content': prompt.strip(),
                            'images': [image_data_processed]
                        }
                    ]

                # ä½¿ç”¨Ollama SDKåˆ†æå›¾ç‰‡
                response = client.chat(
                    model=self.model,
                    messages=messages,
                    options={
                        'temperature': temperature,
                        'num_predict': max_tokens
                    }
                )

                # å¤„ç†å“åº”
                response_text = ""
                if isinstance(response, dict):
                    message = response.get('message', {})
                    if isinstance(message, dict):
                        response_text = message.get('content', '')
                elif hasattr(response, 'message') and hasattr(response.message, 'content'):
                    response_text = response.message.content

                if response_text:
                    status_msg = f"Successfully analyzed image"
                    return response_text, status_msg
                else:
                    return "", "Error: Empty response from vision model"

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä»…åœ¨ébase64æ¨¡å¼ä¸‹ï¼‰
                if not self.use_base64:
                    self._cleanup_temp_file(image_data_processed)

        except (ResponseError, RequestError) as e:
            error_msg = f"Ollama API error: {e}"
            return "", f"Image analysis failed: {error_msg}"
        except Exception as e:
            error_msg = f"Image analysis error: {type(e).__name__}: {e}"
            return "", error_msg

    def analyze_multiple_images(self, prompt: str, images_data: List, system_prompt: str = "You are a helpful assistant.",
                                temperature: float = 0.7, max_tokens: int = 500) -> Tuple[str, str]:
        """
        åˆ†æå¤šå¼ å›¾ç‰‡ / Analyze multiple images in a single request

        Args:
            prompt: å›¾ç‰‡åˆ†ææç¤ºè¯
            images_data: å›¾ç‰‡æ•°æ®åˆ—è¡¨ (List[torch.Tensor]æˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: ç”Ÿæˆæ¸©åº¦ (0.0-2.0)
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            Tuple[str, str]: (åˆ†æç»“æœ, çŠ¶æ€ä¿¡æ¯)
        """
        try:
            # éªŒè¯è¾“å…¥
            if not prompt or not prompt.strip():
                return "", "Error: Empty prompt"

            if not images_data or len(images_data) == 0:
                return "", "Error: No images data provided"

            # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½
            if not self.is_vision_model():
                return "", f"Error: Model '{self.model}' does not support vision. Please use a vision model."

            # é™åˆ¶å‚æ•°èŒƒå›´ (å¤šå›¾ç‰‡æ—¶ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°)
            temperature = max(0.0, min(1.0, float(temperature)))
            max_tokens = max(1, min(8192, int(max_tokens)))

            # é™åˆ¶å›¾ç‰‡æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
            if len(images_data) > 10:
                return "", "Error: Too many images provided (maximum 10 allowed per request)"

            # æ£€æŸ¥è¿æ¥
            if not self._connected:
                if not self.test_connection():
                    return "", "Error: Failed to connect to Ollama server"

            if not self._available_models:
                return "", "Error: No models available on server"

            # å‡†å¤‡æ‰€æœ‰å›¾ç‰‡æ•°æ®
            image_data_list = []
            temp_files = []

            for i, image_data in enumerate(images_data):
                if image_data is None:
                    continue

                image_data_processed = self._prepare_image_for_sdk(image_data)
                if image_data_processed:
                    image_data_list.append(image_data_processed)
                    # å¦‚æœä¸æ˜¯base64æ¨¡å¼ä¸”æ˜¯ä¸´æ—¶æ–‡ä»¶ï¼Œè®°å½•ä¸‹æ¥ä»¥ä¾¿æ¸…ç†
                    if not self.use_base64 and isinstance(image_data_processed, str) and os.path.exists(image_data_processed):
                        temp_files.append(image_data_processed)

            if not image_data_list:
                return "", "Error: Failed to prepare any images for analysis"

            try:
                client = self._get_client()

                # å‡†å¤‡æ¶ˆæ¯ - åŒ…å«å¤šå¼ å›¾ç‰‡
                messages = [
                    {
                        'role': 'system',
                        'content': system_prompt.strip()
                    },
                    {
                        'role': 'user',
                        'content': prompt.strip(),
                        'images': image_data_list
                    }
                ]

                # ä½¿ç”¨Ollama SDKåˆ†æå¤šå¼ å›¾ç‰‡
                response = client.chat(
                    model=self.model,
                    messages=messages,
                    options={
                        'temperature': temperature,
                        'num_predict': max_tokens
                    }
                )

                # å¤„ç†å“åº”
                response_text = ""
                if isinstance(response, dict):
                    message = response.get('message', {})
                    if isinstance(message, dict):
                        response_text = message.get('content', '')
                elif hasattr(response, 'message') and hasattr(response.message, 'content'):
                    response_text = response.message.content

                if response_text:
                    status_msg = f"Successfully analyzed {len(image_data_list)} images"
                    return response_text, status_msg
                else:
                    return "", "Error: Empty response from vision model"

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä»…åœ¨ébase64æ¨¡å¼ä¸‹ï¼‰
                if not self.use_base64:
                    for temp_file in temp_files:
                        self._cleanup_temp_file(temp_file)

        except (ResponseError, RequestError) as e:
            error_msg = f"Ollama API error: {e}"
            return "", f"Multi-image analysis failed: {error_msg}"
        except Exception as e:
            error_msg = f"Multi-image analysis error: {type(e).__name__}: {e}"
            return "", error_msg

    def _prepare_image_for_sdk(self, image_data) -> Optional[str]:
        """
        ä¸ºOllama SDKå‡†å¤‡å›¾ç‰‡ / Prepare image for Ollama SDK

        Args:
            image_data: å›¾ç‰‡æ•°æ® (torch.Tensoræˆ–æ–‡ä»¶è·¯å¾„æˆ–base64å­—ç¬¦ä¸²)

        Returns:
            Optional[str]: base64å­—ç¬¦ä¸²æˆ–æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not PIL_AVAILABLE:
            error_msg = "Error: PIL not available for image processing"
            print(error_msg)
            return None

        try:
            # å¤„ç†torch.Tensor
            if isinstance(image_data, torch.Tensor):
                if self.use_base64:
                    return self._tensor_to_base64(image_data)
                else:
                    return self._tensor_to_temp_file(image_data)

            # å¤„ç†æ–‡ä»¶è·¯å¾„
            elif isinstance(image_data, str):
                if os.path.exists(image_data):
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡
                    if self._is_valid_image_file(image_data):
                        if self.use_base64:
                            # å°†æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸ºbase64
                            with Image.open(image_data) as img_pil:
                                return self._pil_to_base64(img_pil)
                        else:
                            return image_data
                    else:
                        print(f"Error: File exists but is not a valid image: {image_data}")
                        return None
                else:
                    # å°è¯•è§£ç base64
                    if self._is_base64_string(image_data):
                        if self.use_base64:
                            return image_data  # ç›´æ¥è¿”å›base64å­—ç¬¦ä¸²
                        else:
                            # å°†base64è½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶
                            try:
                                img_bytes = base64.b64decode(image_data)
                                img_pil = Image.open(io.BytesIO(img_bytes))
                                return self._pil_to_temp_file(img_pil)
                            except Exception as e:
                                print(f"Failed to decode base64 image: {e}")
                                return None
                    else:
                        print(f"Error: File does not exist and is not valid base64: {image_data}")
                        return None

            else:
                print(f"Error: Unsupported image data type: {type(image_data)}")
                return None

        except Exception as e:
            print(f"Error preparing image: {e}")
            return None

    def _is_valid_image_file(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡ / Check if file is a valid image

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡
        """
        try:
            with Image.open(file_path) as img:
                # å°è¯•è·å–å›¾ç‰‡å°ºå¯¸æ¥éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                _ = img.size
                return True
        except Exception:
            return False

    def _is_base64_string(self, string: str) -> bool:
        """
        æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºæœ‰æ•ˆçš„base64ç¼–ç  / Check if string is valid base64

        Args:
            string: è¦æ£€æŸ¥çš„å­—ç¬¦ä¸²

        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆçš„base64å­—ç¬¦ä¸²
        """
        try:
            # ç®€å•çš„é•¿åº¦æ£€æŸ¥å’Œå­—ç¬¦éªŒè¯
            if len(string) < 100:  # é€šå¸¸base64å›¾ç‰‡éƒ½æ¯”è¾ƒé•¿
                return False
            base64.b64decode(string, validate=True)
            return True
        except Exception:
            return False

    def _tensor_to_base64(self, tensor: torch.Tensor) -> Optional[str]:
        """å°†torch.Tensorè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²ï¼Œå®Œå…¨ä¿æŒåŸå§‹ä¿¡æ¯ / Convert torch.Tensor to base64 string preserving all original info"""
        try:
            # éªŒè¯tensor
            if not isinstance(tensor, torch.Tensor):
                print(f"Error: Expected torch.Tensor, got {type(tensor)}")
                return None

            if tensor.numel() == 0:
                print("Error: Empty tensor provided")
                return None

            # éªŒè¯å½¢çŠ¶ - åº”è¯¥æ˜¯ [H, W, C] æ ¼å¼çš„å•å¼ å›¾åƒ
            if len(tensor.shape) != 3:
                print(f"Error: Expected 3D tensor [H, W, C], got {len(tensor.shape)}D tensor with shape {tensor.shape}")
                return None

            h, w, c = tensor.shape
            if c not in [1, 3, 4]:
                print(f"Error: Invalid number of channels: {c}")
                return None

            if h < 1 or w < 1:
                print(f"Error: Invalid image dimensions: {h}x{w}")
                return None

            
            # åˆ›å»ºtensorçš„å‰¯æœ¬ç”¨äºè½¬æ¢ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            tensor_copy = tensor.clone().detach()

            # å¤„ç†æ•°æ®ç±»å‹å’Œå€¼èŒƒå›´ï¼Œä»…åšå¿…è¦çš„è½¬æ¢
            if tensor_copy.dtype in [torch.float16, torch.float32, torch.float64]:
                # æµ®ç‚¹æ•°æ•°æ®éœ€è¦ç¡®å®šèŒƒå›´
                if tensor_copy.max() <= 1.0:
                    # å‡è®¾æ˜¯ [0, 1] èŒƒå›´çš„æµ®ç‚¹æ•°ï¼Œè½¬æ¢ä¸º [0, 255]
                    tensor_copy = (tensor_copy * 255).clamp(0, 255)
                else:
                    # å‡è®¾å·²ç»æ˜¯ [0, 255] èŒƒå›´
                    tensor_copy = tensor_copy.clamp(0, 255)
                tensor_copy = tensor_copy.to(torch.uint8)
            elif tensor_copy.dtype != torch.uint8:
                # å…¶ä»–æ•´æ•°ç±»å‹è½¬ä¸º uint8
                tensor_copy = tensor_copy.to(torch.uint8)

            # è½¬æ¢åˆ°CPUå¹¶è½¬ä¸ºnumpyæ•°ç»„
            img_np = tensor_copy.cpu().numpy()

            # å¤„ç†é€šé“æ ¼å¼ï¼Œä¿æŒå…¼å®¹æ€§
            if c == 1:
                # ç°åº¦å›¾è½¬RGBï¼Œå¤åˆ¶é€šé“
                img_np = np.stack([img_np] * 3, axis=2)
            elif c == 4:
                # RGBAè½¬RGBï¼Œä¿ç•™RGBé€šé“
                img_np = img_np[:, :, :3]
            elif c == 3:
                # å·²ç»æ˜¯RGBï¼Œæ— éœ€è½¬æ¢
                pass

            # ç¡®ä¿æœ€ç»ˆæ˜¯3é€šé“RGB
            if img_np.shape[2] != 3:
                print(f"âŒ [SiberiaOllamaSDK] Final image doesn't have 3 channels: {img_np.shape}")
                return None

            # åˆ›å»ºPILå›¾åƒ
            img_pil = Image.fromarray(img_np.astype(np.uint8), mode='RGB')

            # è½¬æ¢ä¸ºbase64
            return self._pil_to_base64(img_pil)

        except Exception as e:
            print(f"Error converting tensor to base64: {e}")
            return None

    def _tensor_to_temp_file(self, tensor: torch.Tensor) -> Optional[str]:
        """å°†torch.Tensorè½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶ / Convert torch.Tensor to temporary file"""
        try:
            # éªŒè¯tensor
            if not isinstance(tensor, torch.Tensor):
                print(f"Error: Expected torch.Tensor, got {type(tensor)}")
                return None

            if tensor.numel() == 0:
                print("Error: Empty tensor provided")
                return None

            # å¤„ç†æ‰¹æ¬¡ç»´åº¦
            if len(tensor.shape) == 5:
                # [N, B, H, W, C] -> [N*H, W, C] é€šè¿‡é‡å¡‘
                if tensor.shape[1] == 1:
                    tensor = tensor.squeeze(1)  # [N, H, W, C]
                print(f"Warning: 5D tensor detected, shape: {tensor.shape}")
            elif len(tensor.shape) == 4:
                # [B, H, W, C] -> [H, W, C] å–ç¬¬ä¸€å¼ å›¾ç‰‡
                if tensor.shape[0] > 1:
                    print(f"Warning: Multiple images in batch, using first image. Batch size: {tensor.shape[0]}")
                tensor = tensor[0]
            elif len(tensor.shape) == 2:
                # [H, W] -> [H, W, 3] ç°åº¦å›¾
                tensor = torch.stack([tensor] * 3, dim=-1)

            # éªŒè¯æœ€ç»ˆå½¢çŠ¶
            if len(tensor.shape) != 3:
                print(f"Error: Invalid tensor shape after processing: {tensor.shape}")
                return None

            h, w, c = tensor.shape
            if c not in [1, 3, 4]:
                print(f"Error: Invalid number of channels: {c}")
                return None

            if h < 1 or w < 1:
                print(f"Error: Invalid image dimensions: {h}x{w}")
                return None

            # ä¸é™åˆ¶å›¾ç‰‡å°ºå¯¸ï¼Œä¿æŒåŸå§‹åˆ†è¾¨ç‡
            print(f"ğŸ“¸ [SiberiaOllamaSDK] Processing image at original resolution: {h}x{w}x{c}")

            # è½¬æ¢æ•°æ®ç±»å‹
            if tensor.dtype in [torch.float32, torch.float64, torch.float16]:
                # å‡è®¾æ•°æ®åœ¨[0,1]èŒƒå›´å†…
                if tensor.max() <= 1.0:
                    tensor = (tensor * 255).clamp(0, 255)
                else:
                    tensor = tensor.clamp(0, 255)
                tensor = tensor.to(torch.uint8)

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_np = tensor.cpu().numpy()

            # å¤„ç†ä¸åŒçš„é€šé“æ ¼å¼
            if len(img_np.shape) == 3:
                if img_np.shape[2] == 1:  # å•é€šé“
                    img_np = np.concatenate([img_np] * 3, axis=2)
                elif img_np.shape[2] == 4:  # RGBA -> RGB
                    img_np = img_np[:, :, :3]
                elif img_np.shape[2] > 3:  # è¶…è¿‡3é€šé“
                    img_np = img_np[:, :, :3]

            # åˆ›å»ºPILå›¾åƒ
            img_pil = Image.fromarray(img_np.astype(np.uint8), mode='RGB')
            return self._pil_to_temp_file(img_pil)

        except Exception as e:
            print(f"Error converting tensor to image: {e}")
            return None

    def _pil_to_base64(self, img_pil) -> Optional[str]:
        """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸² / Convert PIL image to base64 string"""
        try:
            buffer = io.BytesIO()
            # ä½¿ç”¨PNGæ ¼å¼ä»¥ä¿æŒæœ€ä½³å›¾åƒè´¨é‡
            img_pil.save(buffer, format='PNG', compress_level=6)
            img_bytes = buffer.getvalue()
            buffer.close()

            return base64.b64encode(img_bytes).decode('utf-8')
        except Exception as e:
            print(f"Error converting PIL image to base64: {e}")
            return None

    def _pil_to_temp_file(self, img_pil) -> Optional[str]:
        """å°†PILå›¾åƒä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ / Save PIL image as temporary file"""
        try:
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
            temp_file.close()
            img_pil.save(temp_file.name, 'PNG')
            return temp_file.name
        except Exception as e:
            print(f"Error saving PIL image to temp file: {e}")
            return None

    def _cleanup_temp_file(self, file_path: str):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ / Cleanup temporary file"""
        if file_path and os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except OSError as e:
                print(f"Warning: Failed to delete temp file {file_path}: {e}")

    @property
    def connected(self) -> bool:
        """è¿æ¥çŠ¶æ€ / Connection status"""
        return self._connected

    @property
    def available_models(self) -> List[str]:
        """å¯ç”¨æ¨¡å‹åˆ—è¡¨ / Available models list"""
        return self._available_models.copy()

    def to_connection_info(self) -> Dict:
        """
        è½¬æ¢ä¸ºè¿æ¥ä¿¡æ¯å­—å…¸ / Convert to connection info dict
        """
        return {
            "server_url": self.server_url,
            "model": self.model,
            "timeout": self.timeout,
            "use_base64": self.use_base64,
            "available_models": self.available_models,
            "connected": self.connected
        }

    @classmethod
    def from_connection_info(cls, connection_info: Dict) -> 'SiberiaOllamaSDKClient':
        """
        ä»è¿æ¥ä¿¡æ¯åˆ›å»ºå®¢æˆ·ç«¯ / Create client from connection info
        """
        if not connection_info:
            return cls()

        client = cls(
            server_url=connection_info.get("server_url", "http://127.0.0.1:11434"),
            model=connection_info.get("model", "llama2"),
            timeout=connection_info.get("timeout", 30),
            use_base64=connection_info.get("use_base64", False)
        )

        # æ¢å¤è¿æ¥çŠ¶æ€
        client._connected = connection_info.get("connected", False)
        client._available_models = connection_info.get("available_models", [])

        return client